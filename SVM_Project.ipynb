{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM Dectection using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Add intelex accelerator to speed up SVM algorithm</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearnx\n",
    "#patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_DS1 = \"Datasets/glasses-and-coverings/\"\n",
    "FOLDER_DS2 = \"Datasets/glasses-and-noglasses/\"\n",
    "FOLDER = \"Datasets/combined_datasets/\"\n",
    "# folders within glasses-and-covering\n",
    "covering = \"covering\"\n",
    "glasses_ds1 = \"glasses_ds1\"\n",
    "plain = \"plain\"\n",
    "sunglasses = \"sunglasses\"\n",
    "sunglasses_imagenet = \"sunglasses-imagenet\"\n",
    "# folder within glasses-and-noglasses\n",
    "glasses_ds2 = \"glasses_ds2\"\n",
    "no_glasses = \"no_glasses\"\n",
    "#Combined dataset\n",
    "glasses ='glasses'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check paths are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = os.path.join(FOLDER_DS2,glasses_ds2)\n",
    "print(os.listdir(pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Inner folder names</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders we are interested in\n",
    "folder_ds1 = [glasses_ds1,plain,sunglasses,sunglasses_imagenet]\n",
    "folder_ds2 = [glasses_ds2, no_glasses]\n",
    "folder_combined = [glasses]\n",
    "pth = os.path.join(FOLDER_DS2,folder_ds2[0])\n",
    "print(os.listdir(pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create function to explore data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#Create function to explore datasets\n",
    "def getfolderinfo(FOLDER_PATH,folders):\n",
    "    # Define a default dict to store folder properties\n",
    "    folder_info = defaultdict(dict)\n",
    "    for folder in folders:\n",
    "        path = os.path.join(FOLDER_PATH, folder)\n",
    "        length = len(os.listdir(path))\n",
    "        folder_info[folder]['length'] = length\n",
    "        suffix_count = defaultdict(int)\n",
    "        not_image = 0\n",
    "        # Check files in folder\n",
    "        for img in os.listdir(path):\n",
    "            # Check suffix\n",
    "            suffix = img.split('.')[-1]\n",
    "            # Define these suffix as valid image files\n",
    "            if suffix.lower() in ['jpg', 'jpeg', 'png', 'gif']:\n",
    "                suffix_count[suffix] += 1\n",
    "            else:\n",
    "                not_image += 1\n",
    "\n",
    "            # Save folder info            \n",
    "            folder_info[folder]['suffix_frequency'] = suffix_count\n",
    "            folder_info[folder]['not_image'] = not_image\n",
    "\n",
    "    return folder_info\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Execute function on dataset folders</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_info_ds1 = getfolderinfo(FOLDER_DS1,folder_ds1)\n",
    "folder_info_ds2 = getfolderinfo(FOLDER_DS2, folder_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glasses-and-coverings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualise results.<br>These results shows that the dataset is balanced in terms of numbers of images so there is not imbalance in the data which not cause bias.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(folder_info_ds1.keys())\n",
    "num_images = [folder_info_ds1[folder]['length'] for folder in keys]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(keys, num_images, color='skyblue')\n",
    "plt.xlabel('Folders')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images in each Folder of Dataset 1')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> All files in folders are images with most files being jpg in this dataset with small exception. This should cause no problems going forward.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_image_count = 0\n",
    "\n",
    "suffix_counts = defaultdict(int)\n",
    "not_image_count = 0\n",
    "\n",
    "for folder_data in folder_info_ds1.values():\n",
    "    suffix_info = folder_data['suffix_frequency']\n",
    "    for suffix, count in suffix_info.items():\n",
    "        suffix_counts[suffix] += count\n",
    "    not_image_count += folder_data['not_image']\n",
    "\n",
    "suffix_labels = list(suffix_counts.keys())\n",
    "suffix_freqs = list(suffix_counts.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(suffix_labels, suffix_freqs, color='skyblue')\n",
    "plt.bar('Not Image', not_image_count, color='lightgreen')\n",
    "plt.xlabel('Suffix or Not Image')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Frequency of Suffixes and Non-Image Files of Dataset 1')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glasses-and-noglasses dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualise results<br>\n",
    "Dataset is fairly balanced with a slight bias towards images with glasses with the difference about 16%. This may cause a slight bias but these datasets will be combined so the variance can be rechecked then.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = list(folder_info_ds2.keys())\n",
    "num_images = [folder_info_ds2[folder]['length'] for folder in folders]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(folders, num_images, color='skyblue')\n",
    "plt.xlabel('Folders')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images in each Folder of Dataset 2')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> All files in folders are images with all files being jpg in this dataset. This should cause no problems going forward.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_image_count = 0\n",
    "\n",
    "suffix_counts = defaultdict(int)\n",
    "not_image_count = 0\n",
    "\n",
    "for folder_data in folder_info_ds2.values():\n",
    "    suffix_info = folder_data['suffix_frequency']\n",
    "    for suffix, count in suffix_info.items():\n",
    "        suffix_counts[suffix] += count\n",
    "    not_image_count += folder_data['not_image']\n",
    "\n",
    "suffix_labels = list(suffix_counts.keys())\n",
    "suffix_freqs = list(suffix_counts.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(suffix_labels, suffix_freqs, color='skyblue')\n",
    "plt.bar('Not Image', not_image_count, color='lightgreen')\n",
    "plt.xlabel('Suffix or Not Image')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Frequency of Suffixes and Non-Image Files of Dataset 2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [plain, glasses, sunglasses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualise results<br>\n",
    "Combining these datasets leads to a large imbalance in numbers of images with sunglasses. This has the potential to bias the model against the sunglasses class. To rectify, when creating training data, we could take all images from sunglasses and equal images from glasses and plain.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_size = []\n",
    "for folder in folders:\n",
    "    path = os.path.join(FOLDER, folder)\n",
    "    length = len(os.listdir(path))\n",
    "    combined_size.append(length)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(folders, combined_size, color='skyblue')\n",
    "plt.xlabel('Folders')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images in each Folder of Combined dataset')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sample Images</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = os.listdir(os.path.join(FOLDER,'glasses'))\n",
    "pth = os.path.join(FOLDER,'glasses', lists[0])\n",
    "img_array = cv2.imread(pth, cv2.IMREAD_GRAYSCALE)\n",
    "backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "plt.imshow(img_array, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    lists = os.listdir(os.path.join(FOLDER,'glasses'))\n",
    "    pth = os.path.join(FOLDER,'glasses', lists[i])\n",
    "    img_array = cv2.imread(pth, cv2.IMREAD_GRAYSCALE)\n",
    "    backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "    axi.imshow(img_array, cmap=\"gray\")\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel='glasses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Define labels</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ glasses, plain, sunglasses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create function to process images to training data<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading all the images and converting to array for data and labels\n",
    "\n",
    "def create_training_data(FOLDER, folders):\n",
    "    training_data = []\n",
    "    for folder in folders:\n",
    "        path = os.path.join(FOLDER, folder)\n",
    "        class_num = folders.index(folder) #0,1\n",
    "        for img in os.listdir(path)[:len(os.listdir(path))]:\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path + \"\\\\\" + img), cv2.IMREAD_GRAYSCALE)\n",
    "                backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "                new_array = cv2.resize(backtorgb, (256, 256))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Combined dataset training data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = create_training_data( FOLDER, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in combined_dataset:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Flatten image for each entry to be a single column with however many columns are needed (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> First model, simple Support Vector Machine with kernel set to rbf. Class_weight set to balanced.<br>\n",
    "Kernel: RBF Radial Balance Function-effective for non-linear relationships in data.<br>\n",
    "Class_weight: Can be unsed to balance imbalanced class distributions. eg. sunglasses.</p?>\n",
    " <p>Result: Dataset was too large for SVM. SVMs have poor performance on large dataset due to the exponential calculations as the dataset grows.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset too large for model\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "#model = SVC(kernel='rbf', class_weight='balanced')\n",
    "#model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Next model, use Principal Component Analysis to see if this can reduce the data the SVM must calculate a fit for. This model created a fit in around 120 seconds compared to the previous which ran for 20 minutes before stopping execution. This shows that using PCA is essential when utilizing the standard SVM algorithm on large datasets, <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Set components to 150\n",
    "pca = PCA(n_components=150, whiten=True,\n",
    "          svd_solver='randomized', random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We can see from these results that the model performs well on glasses and no glasses but poorly on sunglasses. So the imbalance in the dataset does indeed seemed to cause bias in our model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cbar=False, cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Model 3: In this model we will attempt to improved accuracy on sunglasses using the class_weight property of the SVM model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First we need to calculation weights for each class</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sum = sum (combined_size) \n",
    "#float(combined_sum)\n",
    "combined_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = len(combined_size)\n",
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights=[]\n",
    "for i, size in enumerate(combined_size):\n",
    "    class_weights.append( str(combined_sum / (entries * size)))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apparently, it must be a  dict\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1], 2: class_weights[2]} \n",
    "class_weights_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Then add the class weights to model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150, whiten=True,\n",
    "          svd_solver='randomized', random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight=class_weights_dict)\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from  sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cbar=False, cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cbar=False, cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With no PCA, lower accuracy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', class_weight='balanced')\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
